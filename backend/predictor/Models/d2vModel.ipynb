{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    dataset = pd.read_csv(filename)\n",
    "\n",
    "    dataset_text_a = dataset.iloc[:, 1]\n",
    "    dataset_text_b = dataset.iloc[:, 2]\n",
    "\n",
    "    return [dataset_text_a, dataset_text_b]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "def process_data(corpus):\n",
    "    # Create a Set of frequent words\n",
    "    stoplist = set('a e o ou à'.split(' '))\n",
    "\n",
    "    # Words that needs to be withdrawned\n",
    "    excluded_char = '\"!@#$%¨&*()_+-=`{^}:><?,.;/]~[|'\n",
    "\n",
    "    # Lowercase each document, split it by white spaces and filter out stoplist words\n",
    "    data = [\n",
    "        [word for word in document.lower().translate({ord(i): None for i in excluded_char}).split() if word not in stoplist]\n",
    "        for document in corpus\n",
    "    ]\n",
    "    data = np.array(data, dtype = object)\n",
    "\n",
    "    # Transform Data in a One Dimen array\n",
    "    flatten = list(chain.from_iterable(data))\n",
    "\n",
    "    # Count words frequencies\n",
    "    frequency = defaultdict(int)\n",
    "    frequency = {text: flatten.count(text) for text in flatten}\n",
    "\n",
    "    # Only keep words that appears more than once\n",
    "    processed_data = [[token for token in text if frequency[token] > 1] for text in data]\n",
    "\n",
    "    return processed_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "data = read_data('./Data/train.csv')\n",
    "proc = process_data(data[0])\n",
    "dict = corpora.Dictionary(proc)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "''' Using Doc2Vec Model '''\n",
    "from gensim import models, similarities\n",
    "from gensim.models.doc2vec import TaggedDocument"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'words'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[64], line 9\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# Train Model\u001B[39;00m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;66;03m# model = models.Doc2Vec(bow_corpus, vector_size = 5, window = 2, min_count = 1, workers = 4)\u001B[39;00m\n\u001B[0;32m      8\u001B[0m words \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msystem minors\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mlower()\u001B[38;5;241m.\u001B[39msplit()\n\u001B[1;32m----> 9\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mmodels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDoc2Vec\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbow_corpus\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Progams\\Anaconda\\envs\\TextSimilarityAPI\\lib\\site-packages\\gensim\\models\\doc2vec.py:296\u001B[0m, in \u001B[0;36mDoc2Vec.__init__\u001B[1;34m(self, documents, corpus_file, vector_size, dm_mean, dm, dbow_words, dm_concat, dm_tag_count, dv, dv_mapfile, comment, trim_rule, callbacks, window, epochs, shrink_windows, **kwargs)\u001B[0m\n\u001B[0;32m    292\u001B[0m \u001B[38;5;66;03m# EXPERIMENTAL lockf feature; create minimal no-op lockf arrays (1 element of 1.0)\u001B[39;00m\n\u001B[0;32m    293\u001B[0m \u001B[38;5;66;03m# advanced users should directly resize/adjust as desired after any vocab growth\u001B[39;00m\n\u001B[0;32m    294\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdv\u001B[38;5;241m.\u001B[39mvectors_lockf \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mones(\u001B[38;5;241m1\u001B[39m, dtype\u001B[38;5;241m=\u001B[39mREAL)  \u001B[38;5;66;03m# 0.0 values suppress word-backprop-updates; 1.0 allows\u001B[39;00m\n\u001B[1;32m--> 296\u001B[0m \u001B[38;5;28msuper\u001B[39m(Doc2Vec, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\n\u001B[0;32m    297\u001B[0m     sentences\u001B[38;5;241m=\u001B[39mcorpus_iterable,\n\u001B[0;32m    298\u001B[0m     corpus_file\u001B[38;5;241m=\u001B[39mcorpus_file,\n\u001B[0;32m    299\u001B[0m     vector_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvector_size,\n\u001B[0;32m    300\u001B[0m     sg\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m dm) \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m2\u001B[39m,\n\u001B[0;32m    301\u001B[0m     null_word\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdm_concat,\n\u001B[0;32m    302\u001B[0m     callbacks\u001B[38;5;241m=\u001B[39mcallbacks,\n\u001B[0;32m    303\u001B[0m     window\u001B[38;5;241m=\u001B[39mwindow,\n\u001B[0;32m    304\u001B[0m     epochs\u001B[38;5;241m=\u001B[39mepochs,\n\u001B[0;32m    305\u001B[0m     shrink_windows\u001B[38;5;241m=\u001B[39mshrink_windows,\n\u001B[0;32m    306\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    307\u001B[0m )\n",
      "File \u001B[1;32mD:\\Progams\\Anaconda\\envs\\TextSimilarityAPI\\lib\\site-packages\\gensim\\models\\word2vec.py:429\u001B[0m, in \u001B[0;36mWord2Vec.__init__\u001B[1;34m(self, sentences, corpus_file, vector_size, alpha, window, min_count, max_vocab_size, sample, seed, workers, min_alpha, sg, hs, negative, ns_exponent, cbow_mean, hashfxn, epochs, null_word, trim_rule, sorted_vocab, batch_words, compute_loss, callbacks, comment, max_final_vocab, shrink_windows)\u001B[0m\n\u001B[0;32m    427\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m corpus_iterable \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m corpus_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    428\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_corpus_sanity(corpus_iterable\u001B[38;5;241m=\u001B[39mcorpus_iterable, corpus_file\u001B[38;5;241m=\u001B[39mcorpus_file, passes\u001B[38;5;241m=\u001B[39m(epochs \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m))\n\u001B[1;32m--> 429\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbuild_vocab\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcorpus_iterable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcorpus_iterable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcorpus_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcorpus_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrim_rule\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrim_rule\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    430\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain(\n\u001B[0;32m    431\u001B[0m         corpus_iterable\u001B[38;5;241m=\u001B[39mcorpus_iterable, corpus_file\u001B[38;5;241m=\u001B[39mcorpus_file, total_examples\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcorpus_count,\n\u001B[0;32m    432\u001B[0m         total_words\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcorpus_total_words, epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mepochs, start_alpha\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39malpha,\n\u001B[0;32m    433\u001B[0m         end_alpha\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmin_alpha, compute_loss\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_loss, callbacks\u001B[38;5;241m=\u001B[39mcallbacks)\n\u001B[0;32m    434\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\Progams\\Anaconda\\envs\\TextSimilarityAPI\\lib\\site-packages\\gensim\\models\\doc2vec.py:882\u001B[0m, in \u001B[0;36mDoc2Vec.build_vocab\u001B[1;34m(self, corpus_iterable, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001B[0m\n\u001B[0;32m    841\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mbuild_vocab\u001B[39m(\n\u001B[0;32m    842\u001B[0m         \u001B[38;5;28mself\u001B[39m, corpus_iterable\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, corpus_file\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, update\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, progress_per\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10000\u001B[39m,\n\u001B[0;32m    843\u001B[0m         keep_raw_vocab\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, trim_rule\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[0;32m    844\u001B[0m     ):\n\u001B[0;32m    845\u001B[0m     \u001B[38;5;124;03m\"\"\"Build vocabulary from a sequence of documents (can be a once-only generator stream).\u001B[39;00m\n\u001B[0;32m    846\u001B[0m \n\u001B[0;32m    847\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    880\u001B[0m \n\u001B[0;32m    881\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 882\u001B[0m     total_words, corpus_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mscan_vocab\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    883\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcorpus_iterable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcorpus_iterable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcorpus_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcorpus_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    884\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprogress_per\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprogress_per\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrim_rule\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrim_rule\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    885\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    886\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcorpus_count \u001B[38;5;241m=\u001B[39m corpus_count\n\u001B[0;32m    887\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcorpus_total_words \u001B[38;5;241m=\u001B[39m total_words\n",
      "File \u001B[1;32mD:\\Progams\\Anaconda\\envs\\TextSimilarityAPI\\lib\\site-packages\\gensim\\models\\doc2vec.py:1054\u001B[0m, in \u001B[0;36mDoc2Vec.scan_vocab\u001B[1;34m(self, corpus_iterable, corpus_file, progress_per, trim_rule)\u001B[0m\n\u001B[0;32m   1051\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m corpus_file \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1052\u001B[0m     corpus_iterable \u001B[38;5;241m=\u001B[39m TaggedLineDocument(corpus_file)\n\u001B[1;32m-> 1054\u001B[0m total_words, corpus_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_scan_vocab\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcorpus_iterable\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprogress_per\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrim_rule\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1056\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\n\u001B[0;32m   1057\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcollected \u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m word types and \u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m unique tags from a corpus of \u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m examples and \u001B[39m\u001B[38;5;132;01m%i\u001B[39;00m\u001B[38;5;124m words\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1058\u001B[0m     \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mraw_vocab), \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdv), corpus_count, total_words,\n\u001B[0;32m   1059\u001B[0m )\n\u001B[0;32m   1061\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m total_words, corpus_count\n",
      "File \u001B[1;32mD:\\Progams\\Anaconda\\envs\\TextSimilarityAPI\\lib\\site-packages\\gensim\\models\\doc2vec.py:956\u001B[0m, in \u001B[0;36mDoc2Vec._scan_vocab\u001B[1;34m(self, corpus_iterable, progress_per, trim_rule)\u001B[0m\n\u001B[0;32m    954\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m document_no, document \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(corpus_iterable):\n\u001B[0;32m    955\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m checked_string_types:\n\u001B[1;32m--> 956\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[43mdocument\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwords\u001B[49m, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    957\u001B[0m             logger\u001B[38;5;241m.\u001B[39mwarning(\n\u001B[0;32m    958\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEach \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwords\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m should be a list of words (usually unicode strings). \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    959\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFirst \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mwords\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m here is instead plain \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    960\u001B[0m                 \u001B[38;5;28mtype\u001B[39m(document\u001B[38;5;241m.\u001B[39mwords),\n\u001B[0;32m    961\u001B[0m             )\n\u001B[0;32m    962\u001B[0m         checked_string_types \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'list' object has no attribute 'words'"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "\n",
    "# bow_corpus = [TaggedDocument(doc, [i]) for doc, i in enumerate(dict)]\n",
    "bow_corpus = [dict.doc2bow(text) for text in proc]\n",
    "\n",
    "# Train Model\n",
    "# model = models.Doc2Vec(bow_corpus, vector_size = 5, window = 2, min_count = 1, workers = 4)\n",
    "model = models.Doc2Vec(bow_corpus)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sys import getsizeof\n",
    "\n",
    "def generator_split(x, chuck_size):\n",
    "    for i in range(0, len(x), chuck_size):\n",
    "        yield x[i: i + chuck_size]\n",
    "\n",
    "def process_chunk(chunk):\n",
    "    pass\n",
    "\n",
    "def save_processed(processed):\n",
    "    pass\n",
    "\n",
    "one_m = 10**6\n",
    "x = [x for x in range(one_m)]\n",
    "gen = generator_split(x, 100)\n",
    "\n",
    "for chunk in gen:\n",
    "    print(chunk)\n",
    "    processed = process_chunk(chunk)\n",
    "    save_processed(processed)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
